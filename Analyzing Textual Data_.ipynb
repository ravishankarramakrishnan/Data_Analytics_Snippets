{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of dividing text into a set of meaningful pieces. These pieces are called tokens. For example, we can divide a chunk of text into words, or we can divide it into sentences. Depending on the task at hand, we can define our own conditions to divide the input text into meaningful tokens. Let's take a look at how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\avtar8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Are you curious about tokenization?', \"Let's see how it works!\", 'We need to analyze a couple of sentences with punctuations to see it in action.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print (\"\\nSentence tokenizer:\")\n",
    "print (sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word punct tokenizer:\n",
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'\", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create a new WordPunct tokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    " \n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print (\"\\nWord punct tokenizer:\")\n",
    "print (word_punct_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we deal with a text document, we encounter different forms of a word. Consider the word \"play\". This word can appear in various forms, such as \"play\", \"plays\", \"player\", \"playing\", and so on. These are basically families of words with similar meanings.\n",
    "\n",
    "During text analysis, it's useful to extract the base form of these words. This will help us in extracting some statistics to analyze the overall text. The goal of stemming is to reduce these different forms into a common base form. This uses a heuristic process to cut off the ends of words to extract the base form. Let's see how to do this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['table', 'probably', 'wolves', 'playing', 'is', 'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different stemmers\n",
    "stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stemmer\n",
    "\n",
    "stemmer_porter = PorterStemmer()\n",
    "stemmer_lancaster = LancasterStemmer()\n",
    "stemmer_snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             WORD          PORTER       LANCASTER        SNOWBALL \n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_row = '{:>16}' * (len(stemmers) + 1)\n",
    "print ('\\n', formatted_row.format('WORD', *stemmers), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             WORD          PORTER       LANCASTER        SNOWBALL \n",
      "\n",
      "           table            tabl            tabl            tabl\n",
      "        probably         probabl            prob         probabl\n",
      "          wolves            wolv            wolv            wolv\n",
      "         playing            play            play            play\n",
      "              is              is              is              is\n",
      "             dog             dog             dog             dog\n",
      "             the             the             the             the\n",
      "         beaches           beach           beach           beach\n",
      "        grounded          ground          ground          ground\n",
      "          dreamt          dreamt          dreamt          dreamt\n",
      "        envision           envis           envid           envis\n"
     ]
    }
   ],
   "source": [
    "# Let's iterate through the list of words and stem them using the three stemmers:\n",
    "print ('\\n', formatted_row.format('WORD', *stemmers), '\\n')\n",
    "\n",
    "for word in words:\n",
    "    stemmed_words = [stemmer_porter.stem(word),\n",
    "        stemmer_lancaster.stem(word),\n",
    "        stemmer_snowball.stem(word)]\n",
    "    print (formatted_row.format(word, *stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the three stemming algorithms is basically the level of strictness with which they operate. If you observe the outputs, you will see that the Lancaster stemmer is stricter than the other two stemmers. The Porter stemmer is the least in terms of strictness and Lancaster is the strictest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of lemmatization is also to reduce words to their base forms, but this is a more structured approach. In the previous recipe, we saw that the base words that we obtained using stemmers don't really make sense. For example, the word \"wolves\" was reduced to \"wolv\", which is not a real word.\n",
    "\n",
    "Lemmatization solves this problem by doing things using a vocabulary and morphological analysis of words. It removes inflectional word endings, such as \"ing\" or \"ed\", and returns the base form of a word. This base form is known as the lemma. If you lemmatize the word \"wolves\", you will get \"wolf\" as the output. The output depends on whether the token is a verb or a noun. Let's take a look at how to do this in this recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['table', 'probably', 'wolves', 'playing', 'is', 'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different lemmatizers\n",
    "lemmatizers = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      "\n",
      "                   table                   table                   table\n",
      "                probably                probably                probably\n",
      "                  wolves                    wolf                  wolves\n",
      "                 playing                 playing                    play\n",
      "                      is                      is                      be\n",
      "                     dog                     dog                     dog\n",
      "                     the                     the                     the\n",
      "                 beaches                   beach                   beach\n",
      "                grounded                grounded                  ground\n",
      "                  dreamt                  dreamt                   dream\n",
      "                envision                envision                envision\n"
     ]
    }
   ],
   "source": [
    "lemmatizer_wordnet = WordNetLemmatizer()\n",
    "\n",
    "formatted_row = '{:>24}' * (len(lemmatizers) + 1)\n",
    "print ('\\n', formatted_row.format('WORD', *lemmatizers), '\\n')\n",
    "\n",
    "for word in words:\n",
    "    lemmatized_words = [lemmatizer_wordnet.lemmatize(word, pos='n'), lemmatizer_wordnet.lemmatize(word, pos='v')]\n",
    "    print (formatted_row.format(word, *lemmatized_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking refers to dividing the input text into pieces, which are based on any random condition. This is different from tokenization in the sense that there are no constraints and the chunks do not need to be meaningful at all. This is used very frequently during text analysis. When you deal with really large text documents, you need to divide it into chunks for further analysis. In this recipe, we will divide the input text into a number of pieces, where each piece has a fixed number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\avtar8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    " \n",
    "import numpy as np\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a text into chunks - divide the text based on spaces\n",
    "def splitter(data, num_words):\n",
    "    words = data.split(' ')\n",
    "    output = []\n",
    "    cur_count = 0\n",
    "    cur_words = []\n",
    "    for word in words:\n",
    "        cur_words.append(word)\n",
    "        cur_count += 1\n",
    "        if cur_count == num_words:\n",
    "            output.append(' '.join(cur_words))\n",
    "            cur_words = []\n",
    "            cur_count = 0\n",
    "    output.append(' '.join(cur_words) )\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    # Read the data from the Brown corpus\n",
    "    data = ' '.join(brown.words()[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks = 6\n"
     ]
    }
   ],
   "source": [
    "# Define     # Number of words in each chunk\n",
    "num_words = 1700\n",
    "chunks = []\n",
    "counter = 0\n",
    "text_chunks = splitter(data, num_words)\n",
    "print (\"Number of text chunks =\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we deal with text documents that contain millions of words, we need to convert them into some kind of numeric representation. The reason for this is to make them usable for machine learning algorithms. These algorithms need numerical data so that they can analyze them and output meaningful information.\n",
    "\n",
    "This is where the bag-of-words approach comes into picture. This is basically a model that learns a vocabulary from all the words in all the documents. After this, it models each document by building a histogram of all the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "#from chunking import splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chunking in d:\\anaconda\\envs\\py35\\lib\\site-packages (0.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip install chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    # Read the data from the Brown corpus\n",
    "    data = ' '.join(brown.words()[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in each chunk\n",
    "num_words = 2000\n",
    "\n",
    "chunks = []\n",
    "counter = 0\n",
    "\n",
    "text_chunks = splitter(data, num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that is based on these text chunks\n",
    "for text in text_chunks:\n",
    "    chunk = {'index': counter, 'text': text}\n",
    "    chunks.append(chunk)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to extract a document term matrix. \n",
    "#This is basically a matrix that counts the number of occurrences of each word in the document. \n",
    "#We will use scikit-learn to do this because it has better provisions as compared to NLTK for this particular task. \n",
    "\n",
    "# Extract document term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=.95)\n",
    "doc_term_matrix = vectorizer.fit_transform([chunk['text'] for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      "['about' 'after' 'against' 'aid' 'all' 'also' 'an' 'and' 'are' 'as' 'at'\n",
      " 'be' 'been' 'before' 'but' 'by' 'committee' 'congress' 'did' 'each'\n",
      " 'education' 'first' 'for' 'from' 'general' 'had' 'has' 'have' 'he'\n",
      " 'health' 'his' 'house' 'in' 'increase' 'is' 'it' 'last' 'made' 'make'\n",
      " 'may' 'more' 'no' 'not' 'of' 'on' 'one' 'only' 'or' 'other' 'out' 'over'\n",
      " 'pay' 'program' 'proposed' 'said' 'similar' 'state' 'such' 'take' 'than'\n",
      " 'that' 'the' 'them' 'there' 'they' 'this' 'time' 'to' 'two' 'under' 'up'\n",
      " 'was' 'were' 'what' 'which' 'who' 'will' 'with' 'would' 'year' 'years']\n"
     ]
    }
   ],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "print (\"\\nVocabulary:\")\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document term matrix:\n",
      "\n",
      "         Word     Chunk-0     Chunk-1     Chunk-2     Chunk-3     Chunk-4 \n",
      "\n",
      "       about           1           1           1           1           3\n",
      "       after           2           3           2           1           3\n",
      "     against           1           2           2           1           1\n",
      "         aid           1           1           1           3           5\n",
      "         all           2           2           5           2           1\n",
      "        also           3           3           3           4           3\n",
      "          an           5           7           5           7          10\n",
      "         and          34          27          36          36          41\n",
      "         are           5           3           6           3           2\n",
      "          as          13           4          14          18           4\n",
      "          at           5           7           9           3           6\n",
      "          be          20          14           7          10          18\n",
      "        been           7           1           6          15           5\n",
      "      before           2           2           1           1           2\n",
      "         but           3           3           2           9           5\n",
      "          by           8          22          15          14          12\n",
      "   committee           2          10           3           1           7\n",
      "    congress           1           1           3           3           1\n",
      "         did           2           1           1           2           2\n",
      "        each           1           1           4           3           1\n",
      "   education           3           2           3           1           1\n",
      "       first           4           1           4           6           3\n",
      "         for          22          19          24          27          20\n",
      "        from           4           5           6           5           5\n",
      "     general           2           2           2           3           6\n",
      "         had           3           2           7           2           6\n",
      "         has          10           2           5          20          11\n",
      "        have           4           4           4           7           5\n",
      "          he           4          13          12          13          29\n",
      "      health           1           1           2           6           1\n",
      "         his          10           6           9           3           7\n",
      "       house           5           7           4           4           2\n",
      "          in          38          27          37          49          45\n",
      "    increase           3           1           1           4           1\n",
      "          is          12           9          12          14           8\n",
      "          it          18          16           5           6           9\n",
      "        last           1           1           5           4           2\n",
      "        made           1           1           7           4           3\n",
      "        make           3           2           1           1           1\n",
      "         may           1           1           2           2           1\n",
      "        more           3           5           4           6           7\n",
      "          no           4           1           1           7           3\n",
      "         not           5           6           3          14           7\n",
      "          of          61          69          76          56          53\n",
      "          on          10          18          14          13          13\n",
      "         one           4           5           3           4           9\n",
      "        only           1           1           1           3           2\n",
      "          or           4           4           5           5           4\n",
      "       other           2           6           7           1           3\n",
      "         out           3           3           3           4           1\n",
      "        over           1           1           5           1           2\n",
      "         pay           2           3           5           4           1\n",
      "     program           2           1           4           4           5\n",
      "    proposed           2           2           1           1           1\n",
      "        said          20          15          11           9          21\n",
      "     similar           1           1           2           1           2\n",
      "       state          12           9           5           5           7\n",
      "        such           2           3           2           4           2\n",
      "        take           2           2           2           2           2\n",
      "        than           2           2           3           5           4\n",
      "        that          27          12          12          17          31\n",
      "         the         143         116         132         136         148\n",
      "        them           2           2           2           3           2\n",
      "       there           9           4           2           6           6\n",
      "        they           3           2           2           7           2\n",
      "        this           8           5           8           9           7\n",
      "        time           2           1           2           3          11\n",
      "          to          50          54          46          49          66\n",
      "         two           3           3           4           1           1\n",
      "       under           3           3           5           3           1\n",
      "          up           2           1           6           5           5\n",
      "         was          13          16          11           6          14\n",
      "        were           2           3           4           5           3\n",
      "        what           1           1           1           1           2\n",
      "       which          13          10           2           2           3\n",
      "         who           6           5           9           4           1\n",
      "        will          14           2           5          11           4\n",
      "        with           4           6           6           9          10\n",
      "       would           8          27          15           7          23\n",
      "        year           2           4           9          10           3\n",
      "       years           1           3           2           2           3\n"
     ]
    }
   ],
   "source": [
    "print (\"\\nDocument term matrix:\")\n",
    "chunk_names = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4']\n",
    "formatted_row = '{:>12}' * (len(chunk_names) + 1)\n",
    "print ('\\n', formatted_row.format('Word', *chunk_names), '\\n')\n",
    "\n",
    "for word, item in zip(vocab, doc_term_matrix.T):\n",
    "    # 'item' is a 'csr_matrix' data structure\n",
    "    output = [str(x) for x in item.data]\n",
    "    print (formatted_row.format(word, *output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of text classification is to categorize text documents into different classes. This is an extremely important analysis technique in NLP. We will use a technique, which is based on a statistic called tf-idf, which stands for term frequency—inverse document frequency. This is an analysis tool that helps us understand how important a word is to a document in a set of documents. This serves as a feature vector that's used to categorize documents. You can learn more about it at http://www.tfidf.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These categories are available as part of the news groups dataset that we just imported\n",
    "\n",
    "category_map = {'misc.forsale': 'Sales', 'rec.motorcycles': 'Motorcycles',\n",
    "        'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography',\n",
    "        'sci.space': 'Space'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# Load the training data based on the categories\n",
    "training_data = fetch_20newsgroups(subset='train',\n",
    "        categories=category_map.keys(), shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training data: (2968, 40605)\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Extracting Features\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_termcounts = vectorizer.fit_transform(training_data.data)\n",
    "print (\"\\nDimensions of training data:\", X_train_termcounts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Input Sentence\n",
    "input_data = [\n",
    "    \"The curveballs of right handed pitchers tend to curve to the left\",\n",
    "    \"Caesar cipher is an ancient form of encryption\",\n",
    "    \"This two-wheeler is really good on slippery roads\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tf-idf transformer object and train it\n",
    "\n",
    "# tf-idf transformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_termcounts)\n",
    "\n",
    "# Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB().fit(X_train_tfidf, training_data.target)\n",
    "\n",
    "# Input data Transformation using word counts\n",
    "X_input_termcounts = vectorizer.transform(input_data)\n",
    "\n",
    "# Transform the input data using the tf-idf transformer\n",
    "X_input_tfidf = tfidf_transformer.transform(X_input_termcounts)\n",
    "\n",
    "# Predict the output categories\n",
    "predicted_categories = classifier.predict(X_input_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: The curveballs of right handed pitchers tend to curve to the left \n",
      "Predicted category: Baseball\n",
      "\n",
      "Input: Caesar cipher is an ancient form of encryption \n",
      "Predicted category: Cryptography\n",
      "\n",
      "Input: This two-wheeler is really good on slippery roads \n",
      "Predicted category: Motorcycles\n"
     ]
    }
   ],
   "source": [
    "# Print the outputs\n",
    "for sentence, category in zip(input_data, predicted_categories):\n",
    "    print ('\\nInput:', sentence, '\\nPredicted category:', \\\n",
    "            category_map[training_data.target_names[category]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Workings of TF-IDF\n",
    "\n",
    "The tf-idf technique is used frequently in information retrieval. The goal is to understand the importance of each word within a document. We want to identify words that are occur many times in a document. At the same time, common words like “is” and “be” don't really reflect the nature of the content. So we need to extract the words that are true indicators. The importance of each word increases as the count increases. At the same time, as it appears a lot, the frequency of this word increases too. These two things tend to balance each other out. We extract the term counts from each sentence. Once we convert this to a feature vector, we train the classifier to categorize these sentences.\n",
    "\n",
    "The term frequency (TF) measures how frequently a word occurs in a given document. As multiple documents differ in length, the numbers in the histogram tend to vary a lot. So, we need to normalize this so that it becomes a level playing field. To achieve normalization, we divide term-frequency by the total number of words in a given document.\n",
    "\n",
    "The inverse document frequency (IDF) measures the importance of a given word. When we compute TF, all words are considered to be equally important. To counter-balance the frequencies of commonly-occurring words, we need to weigh them down and scale up the rare ones. We need to calculate the ratio of the number of documents with the given word and divide it by the total number of documents. IDF is calculated by taking the negative algorithm of this ratio.\n",
    "\n",
    "For example, simple words, such as \"is\" or \"the\" tend to appear a lot in various documents. However, this doesn't mean that we can characterize the document based on these words. At the same time, if a word appears a single time, this is not useful either. So, we look for words that appear a number of times, but not so much that they become noisy. This is formulated in the tf-idf technique and used to classify documents. Search engines frequently use this tool to order the search results by relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the gender of a name is an interesting task in NLP. We will use the heuristic that the last few characters in a name is its defining characteristic. For example, if the name ends with \"la\", it's most likely a female name, such as \"Angela\" or \"Layla\". On the other hand, if the name ends with \"im\", it's most likely a male name, such as \"Tim\" or \"Jim\". As we are sure of the exact number of characters to use, we will experiment with this. Let's see how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\avtar8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\names.zip.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "import nltk\n",
    "nltk.download('names')\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the input word\n",
    "def gender_features(word, num_letters=2):\n",
    "    return {'feature': word[-num_letters:].lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of letters: 1\n",
      "Accuracy ==> 76.2%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> male\n",
      "\n",
      "Number of letters: 2\n",
      "Accuracy ==> 78.60000000000001%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> male\n",
      "\n",
      "Number of letters: 3\n",
      "Accuracy ==> 76.6%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> female\n",
      "\n",
      "Number of letters: 4\n",
      "Accuracy ==> 70.8%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> female\n"
     ]
    }
   ],
   "source": [
    "# Defining the Main Function\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Extract labeled names\n",
    "    labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "            [(name, 'female') for name in names.words('female.txt')])\n",
    "    random.seed(7)\n",
    "    random.shuffle(labeled_names)\n",
    "    input_names = ['Leonardo', 'Amy', 'Sam']\n",
    "    # Sweeping the parameter space\n",
    "    for i in range(1, 5):\n",
    "        print ('\\nNumber of letters:', i)\n",
    "        featuresets = [(gender_features(n, i), gender) for (n, gender) in labeled_names]\n",
    "        # Divide this into train and test datasets\n",
    "        train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "        classifier = NaiveBayesClassifier.train(train_set)\n",
    "        # Print classifier accuracy\n",
    "        print ('Accuracy ==>', str(100 * nltk_accuracy(classifier, test_set)) + str('%'))\n",
    "\n",
    "        # Predict outputs for new inputs\n",
    "        for name in input_names:\n",
    "            print (name, '==>', classifier.classify(gender_features(name, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is one of the most popular applications of NLP. Sentiment analysis refers to the process of determining whether a given piece of text is positive or negative. In some variations, we consider \"neutral\" as a third option. This technique is commonly used to discover how people feel about a particular topic. This is used to analyze sentiments of users in various forms, such as marketing campaigns, social media, e-commerce customers, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\avtar8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(word_list):\n",
    "    return dict([(word, True) for word in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training datapoints: 1600\n",
      "Number of test datapoints: 400\n",
      "\n",
      "Accuracy of the classifier: 0.735\n",
      "\n",
      "Top 10 most informative words:\n",
      "outstanding\n",
      "insulting\n",
      "vulnerable\n",
      "ludicrous\n",
      "uninvolving\n",
      "astounding\n",
      "avoids\n",
      "fascination\n",
      "seagal\n",
      "darker\n",
      "\n",
      "Predictions:\n",
      "\n",
      "Review: It is an amazing movie\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.61\n",
      "\n",
      "Review: This is a dull movie. I would never recommend it to anyone.\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.77\n",
      "\n",
      "Review: The cinematography is pretty great in this movie\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.67\n",
      "\n",
      "Review: The direction was terrible and the story was all over the place\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.63\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Load positive and negative reviews\n",
    "    positive_fileids = movie_reviews.fileids('pos')\n",
    "    negative_fileids = movie_reviews.fileids('neg')\n",
    "    \n",
    "    # Seperating Positive and Negative Fields\n",
    "    \n",
    "    features_positive = [(extract_features(movie_reviews.words(fileids=[f])),\n",
    "            'Positive') for f in positive_fileids]\n",
    "    features_negative = [(extract_features(movie_reviews.words(fileids=[f])),\n",
    "            'Negative') for f in negative_fileids]\n",
    "    \n",
    "    # Split the data into train and test (80/20)\n",
    "    threshold_factor = 0.8\n",
    "    threshold_positive = int(threshold_factor * len(features_positive))\n",
    "    threshold_negative = int(threshold_factor * len(features_negative))\n",
    "\n",
    "    # Extracting the Features\n",
    "    features_train = features_positive[:threshold_positive] + features_negative[:threshold_negative]\n",
    "    features_test = features_positive[threshold_positive:] + features_negative[threshold_negative:]\n",
    "    print (\"\\nNumber of training datapoints:\", len(features_train))\n",
    "    print (\"Number of test datapoints:\", len(features_test))\n",
    "\n",
    "    # Using Naive Bayes to train a Model\n",
    "    \n",
    "    # Train a Naive Bayes classifier\n",
    "    classifier = NaiveBayesClassifier.train(features_train)\n",
    "    print (\"\\nAccuracy of the classifier:\", nltk.classify.util.accuracy(classifier, features_test))\n",
    "    \n",
    "    # Get the Most Informative Word\n",
    "    print (\"\\nTop 10 most informative words:\")\n",
    "    for item in classifier.most_informative_features()[:10]:\n",
    "        print (item[0])\n",
    "        \n",
    "    # Create Sample input reviews\n",
    "    input_reviews = [\n",
    "        \"It is an amazing movie\",\n",
    "        \"This is a dull movie. I would never recommend it to anyone.\",\n",
    "        \"The cinematography is pretty great in this movie\",\n",
    "        \"The direction was terrible and the story was all over the place\"\n",
    "    ]\n",
    "\n",
    "    # Running on Classifiers for Predictions\n",
    "    \n",
    "    print (\"\\nPredictions:\")\n",
    "    for review in input_reviews:\n",
    "        print (\"\\nReview:\", review)\n",
    "        probdist = classifier.prob_classify(extract_features(review.split()))\n",
    "        pred_sentiment = probdist.max()\n",
    "        \n",
    "        # Output\n",
    "        print (\"Predicted sentiment:\", pred_sentiment)\n",
    "        print (\"Probability:\", round(probdist.prob(pred_sentiment), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working of the Classifier\n",
    "\n",
    "We use NLTK's Naive Bayes classifier for our task here. In the feature extractor function, we basically extract all the unique words. However, the NLTK classifier needs the data to be arranged in the form of a dictionary. Hence, we arranged it in such a way that the NLTK classifier object can ingest it.\n",
    "\n",
    "Once we divide the data into training and testing datasets, we train the classifier to categorize the sentences into positive and negative. If you look at the top informative words, you can see that we have words such as \"outstanding\" to indicate positive reviews and words such as \"insulting\" to indicate negative reviews. This is interesting information because it tells us what words are being used to indicate strong reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic modeling refers to the process of identifying hidden patterns in text data. The goal is to uncover some hidden thematic structure in a collection of documents. This will help us in organizing our documents in a better way so that we can use them for analysis. This is an active area of research in NLP. You can learn more about it at http://www.cs.columbia.edu/~blei/topicmodeling.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\avtar8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data\n",
    "def load_data(input_file):\n",
    "    data = []\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line[:-1]) \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to preprocess text\n",
    "class Preprocessor(object):\n",
    "    # Initialize various operators\n",
    "    def __init__(self):\n",
    "        # Create a regular expression tokenizer\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        # get the list of stop words\n",
    "        self.stop_words_english = stopwords.words('english')\n",
    "        # Create a Snowball stemmer\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # Define a processor function that takes care of tokenization, stop word removal, and stemming\n",
    "    # Tokenizing, stop word removal, and stemming\n",
    "    def process(self, input_text):\n",
    "        # Tokenize the string\n",
    "        tokens = self.tokenizer.tokenize(input_text.lower())\n",
    "        \n",
    "        # Remove the stop words\n",
    "        tokens_stopwords = [x for x in tokens if not x in self.stop_words_english]\n",
    "        \n",
    "        # Perform stemming on the tokens\n",
    "        tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords]\n",
    "\n",
    "        # Return the processed tokens\n",
    "        return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most contributing words to the topics:\n",
      "\n",
      "Topic 0 ==> 0.056*\"need\" + 0.033*\"order\" + 0.033*\"younger\" + 0.033*\"talent\"\n",
      "\n",
      "Topic 1 ==> 0.058*\"need\" + 0.035*\"parti\" + 0.035*\"make\" + 0.035*\"sure\"\n"
     ]
    }
   ],
   "source": [
    "# Writing the Main File\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # File containing linewise input data\n",
    "    input_file = 'data_topic_modeling.txt'\n",
    " \n",
    "    # Load data\n",
    "    data = load_data(input_file)\n",
    "    \n",
    "    # Create a preprocessor object\n",
    "    preprocessor = Preprocessor()\n",
    "    \n",
    "    # Create a list for processed documents\n",
    "    processed_tokens = [preprocessor.process(x) for x in data]\n",
    "\n",
    "    # Create a dictionary based on the tokenized documents\n",
    "    dict_tokens = corpora.Dictionary(processed_tokens)\n",
    "    \n",
    "    # Create a document-term matrix\n",
    "    corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]\n",
    "    \n",
    "    # Generate the LDA model based on the corpus we just created\n",
    "    num_topics = 2\n",
    "    num_words = 4\n",
    "    \n",
    "    ldamodel = models.ldamodel.LdaModel(corpus,\n",
    "            num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
    "\n",
    "    # Once this identifies the two topics, we can see how it's separating these two topics by looking at the most-contributed words\n",
    "    \n",
    "    print (\"Most contributing words to the topics:\")\n",
    "    for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
    "        print (\"\\nTopic\", item[0], \"==>\", item[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
