{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of dividing text into a set of meaningful pieces. These pieces are called tokens. For example, we can divide a chunk of text into words, or we can divide it into sentences. Depending on the task at hand, we can define our own conditions to divide the input text into meaningful tokens. Let's take a look at how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\avtar8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Are you curious about tokenization?', \"Let's see how it works!\", 'We need to analyze a couple of sentences with punctuations to see it in action.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print (\"\\nSentence tokenizer:\")\n",
    "print (sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word punct tokenizer:\n",
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'\", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create a new WordPunct tokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    " \n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print (\"\\nWord punct tokenizer:\")\n",
    "print (word_punct_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we deal with a text document, we encounter different forms of a word. Consider the word \"play\". This word can appear in various forms, such as \"play\", \"plays\", \"player\", \"playing\", and so on. These are basically families of words with similar meanings.\n",
    "\n",
    "During text analysis, it's useful to extract the base form of these words. This will help us in extracting some statistics to analyze the overall text. The goal of stemming is to reduce these different forms into a common base form. This uses a heuristic process to cut off the ends of words to extract the base form. Let's see how to do this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['table', 'probably', 'wolves', 'playing', 'is', 'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different stemmers\n",
    "stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stemmer\n",
    "\n",
    "stemmer_porter = PorterStemmer()\n",
    "stemmer_lancaster = LancasterStemmer()\n",
    "stemmer_snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             WORD          PORTER       LANCASTER        SNOWBALL \n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_row = '{:>16}' * (len(stemmers) + 1)\n",
    "print ('\\n', formatted_row.format('WORD', *stemmers), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             WORD          PORTER       LANCASTER        SNOWBALL \n",
      "\n",
      "           table            tabl            tabl            tabl\n",
      "        probably         probabl            prob         probabl\n",
      "          wolves            wolv            wolv            wolv\n",
      "         playing            play            play            play\n",
      "              is              is              is              is\n",
      "             dog             dog             dog             dog\n",
      "             the             the             the             the\n",
      "         beaches           beach           beach           beach\n",
      "        grounded          ground          ground          ground\n",
      "          dreamt          dreamt          dreamt          dreamt\n",
      "        envision           envis           envid           envis\n"
     ]
    }
   ],
   "source": [
    "# Let's iterate through the list of words and stem them using the three stemmers:\n",
    "print ('\\n', formatted_row.format('WORD', *stemmers), '\\n')\n",
    "\n",
    "for word in words:\n",
    "    stemmed_words = [stemmer_porter.stem(word),\n",
    "        stemmer_lancaster.stem(word),\n",
    "        stemmer_snowball.stem(word)]\n",
    "    print (formatted_row.format(word, *stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the three stemming algorithms is basically the level of strictness with which they operate. If you observe the outputs, you will see that the Lancaster stemmer is stricter than the other two stemmers. The Porter stemmer is the least in terms of strictness and Lancaster is the strictest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of lemmatization is also to reduce words to their base forms, but this is a more structured approach. In the previous recipe, we saw that the base words that we obtained using stemmers don't really make sense. For example, the word \"wolves\" was reduced to \"wolv\", which is not a real word.\n",
    "\n",
    "Lemmatization solves this problem by doing things using a vocabulary and morphological analysis of words. It removes inflectional word endings, such as \"ing\" or \"ed\", and returns the base form of a word. This base form is known as the lemma. If you lemmatize the word \"wolves\", you will get \"wolf\" as the output. The output depends on whether the token is a verb or a noun. Let's take a look at how to do this in this recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['table', 'probably', 'wolves', 'playing', 'is', 'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different lemmatizers\n",
    "lemmatizers = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      "\n",
      "                   table                   table                   table\n",
      "                probably                probably                probably\n",
      "                  wolves                    wolf                  wolves\n",
      "                 playing                 playing                    play\n",
      "                      is                      is                      be\n",
      "                     dog                     dog                     dog\n",
      "                     the                     the                     the\n",
      "                 beaches                   beach                   beach\n",
      "                grounded                grounded                  ground\n",
      "                  dreamt                  dreamt                   dream\n",
      "                envision                envision                envision\n"
     ]
    }
   ],
   "source": [
    "lemmatizer_wordnet = WordNetLemmatizer()\n",
    "\n",
    "formatted_row = '{:>24}' * (len(lemmatizers) + 1)\n",
    "print ('\\n', formatted_row.format('WORD', *lemmatizers), '\\n')\n",
    "\n",
    "for word in words:\n",
    "    lemmatized_words = [lemmatizer_wordnet.lemmatize(word, pos='n'), lemmatizer_wordnet.lemmatize(word, pos='v')]\n",
    "    print (formatted_row.format(word, *lemmatized_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking refers to dividing the input text into pieces, which are based on any random condition. This is different from tokenization in the sense that there are no constraints and the chunks do not need to be meaningful at all. This is used very frequently during text analysis. When you deal with really large text documents, you need to divide it into chunks for further analysis. In this recipe, we will divide the input text into a number of pieces, where each piece has a fixed number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\avtar8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    " \n",
    "import numpy as np\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a text into chunks - divide the text based on spaces\n",
    "def splitter(data, num_words):\n",
    "    words = data.split(' ')\n",
    "    output = []\n",
    "    cur_count = 0\n",
    "    cur_words = []\n",
    "    for word in words:\n",
    "        cur_words.append(word)\n",
    "        cur_count += 1\n",
    "        if cur_count == num_words:\n",
    "            output.append(' '.join(cur_words))\n",
    "            cur_words = []\n",
    "            cur_count = 0\n",
    "    output.append(' '.join(cur_words) )\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    # Read the data from the Brown corpus\n",
    "    data = ' '.join(brown.words()[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks = 6\n"
     ]
    }
   ],
   "source": [
    "# Define     # Number of words in each chunk\n",
    "num_words = 1700\n",
    "chunks = []\n",
    "counter = 0\n",
    "text_chunks = splitter(data, num_words)\n",
    "print (\"Number of text chunks =\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we deal with text documents that contain millions of words, we need to convert them into some kind of numeric representation. The reason for this is to make them usable for machine learning algorithms. These algorithms need numerical data so that they can analyze them and output meaningful information.\n",
    "\n",
    "This is where the bag-of-words approach comes into picture. This is basically a model that learns a vocabulary from all the words in all the documents. After this, it models each document by building a histogram of all the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'chunking'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-dd7646d5b7bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mchunking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'chunking'"
     ]
    }
   ],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from chunking import splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    # Read the data from the Brown corpus\n",
    "    data = ' '.join(brown.words()[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in each chunk\n",
    "num_words = 2000\n",
    "\n",
    "chunks = []\n",
    "counter = 0\n",
    "\n",
    "text_chunks = splitter(data, num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that is based on these text chunks\n",
    "for text in text_chunks:\n",
    "    chunk = {'index': counter, 'text': text}\n",
    "    chunks.append(chunk)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to extract a document term matrix. \n",
    "#This is basically a matrix that counts the number of occurrences of each word in the document. \n",
    "#We will use scikit-learn to do this because it has better provisions as compared to NLTK for this particular task. \n",
    "\n",
    "# Extract document term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=.95)\n",
    "doc_term_matrix = vectorizer.fit_transform([chunk['text'] for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      "['about' 'after' 'against' 'aid' 'all' 'also' 'an' 'and' 'are' 'as' 'at'\n",
      " 'be' 'been' 'before' 'but' 'by' 'committee' 'congress' 'did' 'each'\n",
      " 'education' 'first' 'for' 'from' 'general' 'had' 'has' 'have' 'he'\n",
      " 'health' 'his' 'house' 'in' 'increase' 'is' 'it' 'last' 'made' 'make'\n",
      " 'may' 'more' 'no' 'not' 'of' 'on' 'one' 'only' 'or' 'other' 'out' 'over'\n",
      " 'pay' 'program' 'proposed' 'said' 'similar' 'state' 'such' 'take' 'than'\n",
      " 'that' 'the' 'them' 'there' 'they' 'this' 'time' 'to' 'two' 'under' 'up'\n",
      " 'was' 'were' 'what' 'which' 'who' 'will' 'with' 'would' 'year' 'years']\n"
     ]
    }
   ],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "print (\"\\nVocabulary:\")\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document term matrix:\n",
      "\n",
      "         Word     Chunk-0     Chunk-1     Chunk-2     Chunk-3     Chunk-4 \n",
      "\n",
      "       about           1           1           1           1           3\n",
      "       after           2           3           2           1           3\n",
      "     against           1           2           2           1           1\n",
      "         aid           1           1           1           3           5\n",
      "         all           2           2           5           2           1\n",
      "        also           3           3           3           4           3\n",
      "          an           5           7           5           7          10\n",
      "         and          34          27          36          36          41\n",
      "         are           5           3           6           3           2\n",
      "          as          13           4          14          18           4\n",
      "          at           5           7           9           3           6\n",
      "          be          20          14           7          10          18\n",
      "        been           7           1           6          15           5\n",
      "      before           2           2           1           1           2\n",
      "         but           3           3           2           9           5\n",
      "          by           8          22          15          14          12\n",
      "   committee           2          10           3           1           7\n",
      "    congress           1           1           3           3           1\n",
      "         did           2           1           1           2           2\n",
      "        each           1           1           4           3           1\n",
      "   education           3           2           3           1           1\n",
      "       first           4           1           4           6           3\n",
      "         for          22          19          24          27          20\n",
      "        from           4           5           6           5           5\n",
      "     general           2           2           2           3           6\n",
      "         had           3           2           7           2           6\n",
      "         has          10           2           5          20          11\n",
      "        have           4           4           4           7           5\n",
      "          he           4          13          12          13          29\n",
      "      health           1           1           2           6           1\n",
      "         his          10           6           9           3           7\n",
      "       house           5           7           4           4           2\n",
      "          in          38          27          37          49          45\n",
      "    increase           3           1           1           4           1\n",
      "          is          12           9          12          14           8\n",
      "          it          18          16           5           6           9\n",
      "        last           1           1           5           4           2\n",
      "        made           1           1           7           4           3\n",
      "        make           3           2           1           1           1\n",
      "         may           1           1           2           2           1\n",
      "        more           3           5           4           6           7\n",
      "          no           4           1           1           7           3\n",
      "         not           5           6           3          14           7\n",
      "          of          61          69          76          56          53\n",
      "          on          10          18          14          13          13\n",
      "         one           4           5           3           4           9\n",
      "        only           1           1           1           3           2\n",
      "          or           4           4           5           5           4\n",
      "       other           2           6           7           1           3\n",
      "         out           3           3           3           4           1\n",
      "        over           1           1           5           1           2\n",
      "         pay           2           3           5           4           1\n",
      "     program           2           1           4           4           5\n",
      "    proposed           2           2           1           1           1\n",
      "        said          20          15          11           9          21\n",
      "     similar           1           1           2           1           2\n",
      "       state          12           9           5           5           7\n",
      "        such           2           3           2           4           2\n",
      "        take           2           2           2           2           2\n",
      "        than           2           2           3           5           4\n",
      "        that          27          12          12          17          31\n",
      "         the         143         116         132         136         148\n",
      "        them           2           2           2           3           2\n",
      "       there           9           4           2           6           6\n",
      "        they           3           2           2           7           2\n",
      "        this           8           5           8           9           7\n",
      "        time           2           1           2           3          11\n",
      "          to          50          54          46          49          66\n",
      "         two           3           3           4           1           1\n",
      "       under           3           3           5           3           1\n",
      "          up           2           1           6           5           5\n",
      "         was          13          16          11           6          14\n",
      "        were           2           3           4           5           3\n",
      "        what           1           1           1           1           2\n",
      "       which          13          10           2           2           3\n",
      "         who           6           5           9           4           1\n",
      "        will          14           2           5          11           4\n",
      "        with           4           6           6           9          10\n",
      "       would           8          27          15           7          23\n",
      "        year           2           4           9          10           3\n",
      "       years           1           3           2           2           3\n"
     ]
    }
   ],
   "source": [
    "print (\"\\nDocument term matrix:\")\n",
    "chunk_names = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4']\n",
    "formatted_row = '{:>12}' * (len(chunk_names) + 1)\n",
    "print ('\\n', formatted_row.format('Word', *chunk_names), '\\n')\n",
    "\n",
    "for word, item in zip(vocab, doc_term_matrix.T):\n",
    "    # 'item' is a 'csr_matrix' data structure\n",
    "    output = [str(x) for x in item.data]\n",
    "    print (formatted_row.format(word, *output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
